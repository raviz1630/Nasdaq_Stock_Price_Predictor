{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdae622",
   "metadata": {},
   "source": [
    "# Nasdaq Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee897c",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79902871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3933578",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e354ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3180602.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380276.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.50</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166603.91</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.04</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>302879.87</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.00</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11917682.27</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389745.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.90</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.010200</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>447549.96</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.54</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0         0        0                  0      3180602.69   \n",
       "1         1        0                  0       166603.91   \n",
       "2         2        0                  0       302879.87   \n",
       "3         3        0                  0     11917682.27   \n",
       "4         4        0                  0       447549.96   \n",
       "\n",
       "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                        1         0.999812   13380276.64        NaN   \n",
       "1                       -1         0.999896    1642214.25        NaN   \n",
       "2                       -1         0.999561    1819368.03        NaN   \n",
       "3                       -1         1.000171   18389745.62        NaN   \n",
       "4                       -1         0.999532   17860614.95        NaN   \n",
       "\n",
       "   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \\\n",
       "0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704   \n",
       "1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986   \n",
       "2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950   \n",
       "3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200   \n",
       "4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849   \n",
       "\n",
       "   time_id row_id  \n",
       "0        0  0_0_0  \n",
       "1        0  0_0_1  \n",
       "2        0  0_0_2  \n",
       "3        0  0_0_3  \n",
       "4        0  0_0_4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('C:/Users/Sree/Downloads/DL-FInal/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52715b77",
   "metadata": {},
   "source": [
    "#### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010b944",
   "metadata": {},
   "source": [
    "checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97554c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                         0\n",
       "date_id                          0\n",
       "seconds_in_bucket                0\n",
       "imbalance_size                 220\n",
       "imbalance_buy_sell_flag          0\n",
       "reference_price                220\n",
       "matched_size                   220\n",
       "far_price                  2894342\n",
       "near_price                 2857180\n",
       "bid_price                      220\n",
       "bid_size                         0\n",
       "ask_price                      220\n",
       "ask_size                         0\n",
       "wap                            220\n",
       "target                          88\n",
       "time_id                          0\n",
       "row_id                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979f1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaeae8e",
   "metadata": {},
   "source": [
    "Imputation with mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef68e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imbalance_size'].fillna(df['imbalance_size'].median(), inplace=True)\n",
    "df['reference_price'].fillna(df['reference_price'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae32be",
   "metadata": {},
   "source": [
    "Forward fill for time-series variables like far_price and ask_price as we need to see future trends for value filling assumptions for predictive analytics, especially when the missing values are a huge sum of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e8cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['far_price'].fillna(method='ffill', inplace=True)\n",
    "df['near_price'].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad2eee",
   "metadata": {},
   "source": [
    "We still have null values where forward fill didnt work in far_price so we shall fill them up with mean to take the average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a527a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['far_price'].fillna(df['far_price'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de0471",
   "metadata": {},
   "source": [
    "We shall fill the reamining ones with average too in order to least impact the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207ab4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matched_size'].fillna(df['matched_size'].mean(), inplace=True)\n",
    "df['bid_price'].fillna(df['bid_price'].mean(), inplace=True)\n",
    "df['ask_price'].fillna(df['ask_price'].mean(), inplace=True)\n",
    "df['wap'].fillna(df['wap'].mean(), inplace=True)\n",
    "df['target'].fillna(df['target'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e00c0",
   "metadata": {},
   "source": [
    "Checking null values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f1aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                   0\n",
       "date_id                    0\n",
       "seconds_in_bucket          0\n",
       "imbalance_size             0\n",
       "imbalance_buy_sell_flag    0\n",
       "reference_price            0\n",
       "matched_size               0\n",
       "far_price                  0\n",
       "near_price                 0\n",
       "bid_price                  0\n",
       "bid_size                   0\n",
       "ask_price                  0\n",
       "ask_size                   0\n",
       "wap                        0\n",
       "target                     0\n",
       "time_id                    0\n",
       "row_id                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfd7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=['imbalance_buy_sell_flag'], prefix='imbalance_flag')\n",
    "## Convert categorical imbalance_buy_sell_flag to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baf55bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_columns = ['bid_price', 'ask_price', 'wap']\n",
    "train_data[price_columns] = train_data[price_columns] / train_data['wap'].values[:, np.newaxis]\n",
    "# Normalize price-related columns relative to the stock wap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd67ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['imbalance_size', 'imbalance_flag_-1', 'imbalance_flag_0', 'imbalance_flag_1',\n",
    "                      'reference_price', 'matched_size', 'far_price', 'near_price',\n",
    "                      'bid_price', 'ask_price', 'wap', 'seconds_in_bucket']\n",
    "# Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f2075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[selected_features]\n",
    "y = train_data['target']\n",
    "\n",
    "# Select features and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4f951",
   "metadata": {},
   "source": [
    "#### Splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b9e9aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4190384, 12) (4190384,)\n",
      "Testing set shape: (1047596, 12) (1047596,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7862e",
   "metadata": {},
   "source": [
    "#### simple neural network using TensorFlow's Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06eca9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Using MSE for regression\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8233a",
   "metadata": {},
   "source": [
    "#### Training and modifying the built models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6f44695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 64)                832       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.9958 - val_loss: 87.7656\n",
      "Epoch 2/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.7794 - val_loss: 87.6203\n",
      "Epoch 3/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.6869 - val_loss: 87.6376\n",
      "Epoch 4/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.6187 - val_loss: 87.4879\n",
      "Epoch 5/50\n",
      "26190/26190 [==============================] - 40s 2ms/step - loss: 86.5497 - val_loss: 87.4656\n",
      "Epoch 6/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.5090 - val_loss: 87.5170\n",
      "Epoch 7/50\n",
      "26190/26190 [==============================] - 39s 1ms/step - loss: 86.4612 - val_loss: 87.3907\n",
      "Epoch 8/50\n",
      "26190/26190 [==============================] - 42s 2ms/step - loss: 86.4334 - val_loss: 87.2552\n",
      "Epoch 9/50\n",
      "26190/26190 [==============================] - 43s 2ms/step - loss: 86.3735 - val_loss: 87.2935\n",
      "Epoch 10/50\n",
      "26190/26190 [==============================] - 43s 2ms/step - loss: 86.3416 - val_loss: 87.2653\n",
      "Epoch 11/50\n",
      "26190/26190 [==============================] - 44s 2ms/step - loss: 86.3076 - val_loss: 87.1931\n",
      "Epoch 12/50\n",
      "26190/26190 [==============================] - 44s 2ms/step - loss: 86.2782 - val_loss: 87.1197\n",
      "Epoch 13/50\n",
      "26190/26190 [==============================] - 44s 2ms/step - loss: 86.2267 - val_loss: 87.2551\n",
      "Epoch 14/50\n",
      "26190/26190 [==============================] - 45s 2ms/step - loss: 86.1908 - val_loss: 87.1057\n",
      "Epoch 15/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 86.1583 - val_loss: 87.0430\n",
      "Epoch 16/50\n",
      "26190/26190 [==============================] - 45s 2ms/step - loss: 86.1513 - val_loss: 87.0158\n",
      "Epoch 17/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 86.0928 - val_loss: 87.0461\n",
      "Epoch 18/50\n",
      "26190/26190 [==============================] - 45s 2ms/step - loss: 86.0583 - val_loss: 86.9632\n",
      "Epoch 19/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 86.0475 - val_loss: 86.9803\n",
      "Epoch 20/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 86.0123 - val_loss: 86.9188\n",
      "Epoch 21/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.9824 - val_loss: 86.9644\n",
      "Epoch 22/50\n",
      "26190/26190 [==============================] - 45s 2ms/step - loss: 85.9240 - val_loss: 86.9776\n",
      "Epoch 23/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 85.9010 - val_loss: 86.8908\n",
      "Epoch 24/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 85.8973 - val_loss: 86.8535\n",
      "Epoch 25/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.8625 - val_loss: 86.9930\n",
      "Epoch 26/50\n",
      "26190/26190 [==============================] - 46s 2ms/step - loss: 85.8235 - val_loss: 87.0517\n",
      "Epoch 27/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.8293 - val_loss: 86.8953\n",
      "Epoch 28/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.8065 - val_loss: 86.9579\n",
      "Epoch 29/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.7666 - val_loss: 86.8460\n",
      "Epoch 30/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.7691 - val_loss: 86.9356\n",
      "Epoch 31/50\n",
      "26190/26190 [==============================] - 49s 2ms/step - loss: 85.7250 - val_loss: 86.9046\n",
      "Epoch 32/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.7073 - val_loss: 86.7942\n",
      "Epoch 33/50\n",
      "26190/26190 [==============================] - 48s 2ms/step - loss: 85.7261 - val_loss: 86.8098\n",
      "Epoch 34/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.6826 - val_loss: 86.8185\n",
      "Epoch 35/50\n",
      "26190/26190 [==============================] - 48s 2ms/step - loss: 85.6823 - val_loss: 86.9455\n",
      "Epoch 36/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.6850 - val_loss: 86.9462\n",
      "Epoch 37/50\n",
      "26190/26190 [==============================] - 47s 2ms/step - loss: 85.6454 - val_loss: 86.8304\n",
      "32738/32738 [==============================] - 38s 1ms/step - loss: 86.4075\n",
      "Test Loss: 86.40752410888672\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a simpler neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "#Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766b041",
   "metadata": {},
   "source": [
    "#### Improved model with more layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38ab8cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 128)               1664      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12929 (50.50 KB)\n",
      "Trainable params: 12481 (48.75 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "13095/13095 [==============================] - 40s 3ms/step - loss: 86.9522 - val_loss: 87.7284\n",
      "Epoch 2/50\n",
      "13095/13095 [==============================] - 39s 3ms/step - loss: 86.7228 - val_loss: 87.6007\n",
      "Epoch 3/50\n",
      "13095/13095 [==============================] - 40s 3ms/step - loss: 86.5902 - val_loss: 87.5356\n",
      "Epoch 4/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.5209 - val_loss: 87.4275\n",
      "Epoch 5/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.4518 - val_loss: 87.4271\n",
      "Epoch 6/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.4005 - val_loss: 87.4308\n",
      "Epoch 7/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.3444 - val_loss: 87.3000\n",
      "Epoch 8/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.2945 - val_loss: 87.3137\n",
      "Epoch 9/50\n",
      "13095/13095 [==============================] - 37s 3ms/step - loss: 86.2420 - val_loss: 87.1553\n",
      "Epoch 10/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.2039 - val_loss: 87.4793\n",
      "Epoch 11/50\n",
      "13095/13095 [==============================] - 36s 3ms/step - loss: 86.1419 - val_loss: 87.5358\n",
      "Epoch 12/50\n",
      "13095/13095 [==============================] - 40s 3ms/step - loss: 86.0999 - val_loss: 87.1993\n",
      "Epoch 13/50\n",
      "13095/13095 [==============================] - 39s 3ms/step - loss: 86.0492 - val_loss: 87.4596\n",
      "Epoch 14/50\n",
      "13095/13095 [==============================] - 39s 3ms/step - loss: 85.9933 - val_loss: 88.1580\n",
      "32738/32738 [==============================] - 38s 1ms/step - loss: 86.6632\n",
      "Improved Model Test Loss: 86.66320037841797\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization\n",
    "\n",
    "# Improved model with more layers, neurons, and batch normalization\n",
    "improved_model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.BatchNormalization(),  # Batch normalization for improved convergence\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1)  \n",
    "])\n",
    "\n",
    "# Compile the improved model\n",
    "improved_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Display the model summary\n",
    "improved_model.summary()\n",
    "\n",
    "# Train the improved model with early stopping\n",
    "improved_history = improved_model.fit(X_train, y_train, epochs=50, batch_size=256, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the improved model on the test set\n",
    "improved_loss = improved_model.evaluate(X_test, y_test)\n",
    "print(f\"Improved Model Test Loss: {improved_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601f8e5",
   "metadata": {},
   "source": [
    "modifying the model to incorporate LSTM layers, which are well-suited for sequential data like time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d419820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 50)             12600     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1, 50)             200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 50)                200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33251 (129.89 KB)\n",
      "Trainable params: 33051 (129.11 KB)\n",
      "Non-trainable params: 200 (800.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "13095/13095 [==============================] - 64s 5ms/step - loss: 86.9194 - val_loss: 87.7290\n",
      "Epoch 2/50\n",
      "13095/13095 [==============================] - 59s 5ms/step - loss: 86.6938 - val_loss: 87.6579\n",
      "Epoch 3/50\n",
      "13095/13095 [==============================] - 65s 5ms/step - loss: 86.5637 - val_loss: 87.7248\n",
      "Epoch 4/50\n",
      "13095/13095 [==============================] - 65s 5ms/step - loss: 86.4573 - val_loss: 87.3911\n",
      "Epoch 5/50\n",
      "13095/13095 [==============================] - 67s 5ms/step - loss: 86.3567 - val_loss: 87.2783\n",
      "Epoch 6/50\n",
      "13095/13095 [==============================] - 65s 5ms/step - loss: 86.2722 - val_loss: 87.3521\n",
      "Epoch 7/50\n",
      "13095/13095 [==============================] - 62s 5ms/step - loss: 86.1859 - val_loss: 87.2307\n",
      "Epoch 8/50\n",
      "13095/13095 [==============================] - 62s 5ms/step - loss: 86.0999 - val_loss: 86.9526\n",
      "Epoch 9/50\n",
      "13095/13095 [==============================] - 63s 5ms/step - loss: 86.0411 - val_loss: 87.3189\n",
      "Epoch 10/50\n",
      "13095/13095 [==============================] - 60s 5ms/step - loss: 85.9616 - val_loss: 87.2483\n",
      "Epoch 11/50\n",
      "13095/13095 [==============================] - 63s 5ms/step - loss: 85.8807 - val_loss: 87.1625\n",
      "Epoch 12/50\n",
      "13095/13095 [==============================] - 61s 5ms/step - loss: 85.8100 - val_loss: 87.9548\n",
      "Epoch 13/50\n",
      "13095/13095 [==============================] - 62s 5ms/step - loss: 85.7657 - val_loss: 94.2562\n",
      "32738/32738 [==============================] - 48s 1ms/step - loss: 86.6499\n",
      "LSTM Model Test Loss: 86.64994812011719\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n",
    "\n",
    "# Reshape the data to fit the LSTM input shape\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Improved model with LSTM layers\n",
    "lstm_model = models.Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    LSTM(50, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Display the model summary\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the LSTM model with early stopping\n",
    "lstm_history = lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=256, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the LSTM model on the test set\n",
    "lstm_loss = lstm_model.evaluate(X_test_lstm, y_test)\n",
    "print(f\"LSTM Model Test Loss: {lstm_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ad249",
   "metadata": {},
   "source": [
    "#### Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af84fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32738/32738 [==============================] - 37s 1ms/step\n",
      "Accuracy: 0.5645573293521549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = improved_model.predict(X_test)\n",
    "\n",
    "# Define a classification threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# predictions based on the threshold\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "y_test_class = (y_test > threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy in batches\n",
    "batch_size = 10000  \n",
    "num_samples = len(y_test)\n",
    "\n",
    "accuracy = 0.0\n",
    "\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    y_pred_batch = y_pred_class[i:i + batch_size]\n",
    "    y_test_batch = y_test_class[i:i + batch_size]\n",
    "\n",
    "    accuracy += accuracy_score(y_test_batch, y_pred_batch) * len(y_test_batch)\n",
    "\n",
    "accuracy /= num_samples\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cdc03",
   "metadata": {},
   "source": [
    "Generally for regression tasks accuracy might not be the best accuracy, so calculating MSE too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a16f7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32738/32738 [==============================] - 38s 1ms/step\n",
      "Mean Squared Error: 86.6628070065235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = improved_model.predict(X_test)\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the Mean Squared Error\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf292f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
